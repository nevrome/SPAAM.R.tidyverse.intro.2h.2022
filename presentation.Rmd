---
title: "A crash course on R for data analysis"
author: "Clemens Schmid"
date: "2019/06/05"
fontsize: 9pt
output:
  beamer_presentation:
    includes: 
      in_header: preamble.tex
    keep_tex: true
classoption: "aspectratio=169"
fig_caption: yes
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE}
# https://stackoverflow.com/questions/25646333/code-chunk-font-size-in-rmarkdown-with-knitr-and-latex
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(
  echo = TRUE, eval = TRUE, cache = TRUE, results = FALSE, warning = FALSE, message = FALSE,
  fig.align = "center", out.width = "60%", fig.dim = c(7, 4)
)
```

## TOC

- The working environment
- Loading data into tibbles
- Plotting data in tibbles
- Conditional queries on tibbles
- Transforming and manipulating tibbles
- Combining tibbles with join operations

# The working environment

## Getting started for this workshop

- Activate the relevant conda environment (don't forget to deactivate it later!)

```
conda activate r-python
```

- Navigate to

```
/vol/volume/3b-1-introduction-to-r-and-the-tidyverse/spaam_r_tidyverse_intro_2h
```

- Pull the latest changes in this Git repository

```
git pull
```

- Open RStudio
- Load the project with `File > Open Project...`
- Open this file `presentation.Rmd` in RStudio

## R, RStudio and the tidyverse

- R is a fully featured programming language, but it excels as an environment for (statistical) data analysis (https://www.r-project.org)

- RStudio is an integrated development environment (IDE) for R (and other languages): (https://www.rstudio.com/products/rstudio)

- The tidyverse is a collection of packages with well-designed and consistent interfaces for the main steps of data analysis: loading, transforming and plotting data (https://www.tidyverse.org)
  - This introduction works with tidyverse ~v1.3.0
  - We will learn about `readr`, `tibble`, `ggplot2`, `dplyr`, `magrittr` and `tidyr`
  - `forcats` will be briefly mentioned
  - `purrr` and `stringr` are left out

# Loading data into tibbles

## Reading data with readr

```{r echo = FALSE}
sample_table_url <- "https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/ancientmetagenome-hostassociated/samples/ancientmetagenome-hostassociated_samples.tsv"
```

- With R we usually operate on data in our computer's memory
- The tidyverse provides the package `readr` to read data from text files into the memory
- `readr` can read from our file system or the internet
- It provides functions to read data in almost any (text) format:

```{r eval=FALSE}
readr::read_csv()   # .csv files
readr::read_tsv()   # .tsv files
readr::read_delim() # tabular files with an arbitrary separator
readr::read_fwf()   # fixed width files
readr::read_lines() # read linewise to parse yourself 
```

- `readr` automatically detects column types -- but you can also define them manually

## How does the interface of `read_csv` work?

- We can learn more about a function with `?`. To open a help file: `?readr::read_csv`
- `readr::read_csv` has many options to specify how to read a text file

```{r eval=FALSE}
read_csv(
  file,                      # The path to the file we want to read
  col_names = TRUE,          # Are there column names?
  col_types = NULL,          # Which types do the columns have? NULL -> auto
  locale = default_locale(), # How is information encoded in this file?
  na = c("", "NA"),          # Which values mean "no data"
  trim_ws = TRUE,            # Should superfluous white-spaces be removed?
  skip = 0,                  # Skip X lines at the beginning of the file
  n_max = Inf,               # Only read X lines
  skip_empty_rows = TRUE,    # Should empty lines be ignored? 
  comment = "",              # Should comment lines be ignored?
  name_repair = "unique",    # How should "broken" column names be fixed
  ...
)
```

## What does `readr` produce? The `tibble`!

```{r}
samples <- readr::read_tsv(sample_table_url)
```

- The `tibble` is a "data frame", a tabular data structure with rows and columns
- Unlike a simple array, each column can have another data type

```{r results='markup'}
print(samples, n = 3)
```

## How to look at a `tibble`?

```{r, eval=FALSE}
samples          # Typing the name of an object will print it to the console
str(samples)     # A structural overview of an object
summary(samples) # A human-readable summary of an object
View(samples)    # RStudio's interactive data browser
```

- R provides a very flexible indexing operation for `data.frame`s and `tibble`s

```{r, eval=FALSE}
samples[1,1]                         # Access the first row and column
samples[1,]                          # Access the first row
samples[,1]                          # Access the first column
samples[c(1,2,3),c(2,3,4)]           # Access a selection of rows and columns
samples[,-c(1,2)]                    # Remove the first two columns
samples[,c("site_name", "material")] # Columns can be selected by name
```

- `tibble`s are mutable data structures, so their content can be overwritten

```{r, eval=FALSE}
samples[1,1] <- "Cheesecake2015"     # replace the first value in the first column 
```

# Plotting data in `tibble`s

## `ggplot2` and the "grammar of graphics"

- `ggplot2` offers an unusual, but powerful and logical interface
- The following example describes a stacked bar chart

```{r}
library(ggplot2) # Loading a library to use its functions without ::
```

```{r eval=FALSE}
ggplot(          # Every plot starts with a call to the ggplot() function
  data = samples # This function can also take the input tibble
) +              # The plot consists of functions linked with +
geom_bar(        # "geoms" define the plot layers we want to draw
  mapping = aes( # The aes() function maps variables to visual properties
    x = publication_year, # publication_year -> x-axis
    fill = community_type # publication_year -> fill color
  )
)
```

- `geom_*`: data + geometry + statistical transformation + repositioning

## `ggplot2` and the "grammar of graphics"

- This is the plot described above: number of samples per community type through time

```{r}
ggplot(samples) +
geom_bar(aes(x = publication_year, fill = community_type))
```

## `ggplot2` features many geoms

\centering
![](figures/geom_cheatsheet.png){width=55%}

- RStudio shares helpful cheatsheets for the tidyverse and beyond: https://www.rstudio.com/resources/cheatsheets

## `scale`s control the behaviour of visual elements

- Another plot: Boxplots of sample age through time

```{r}
ggplot(samples) +
geom_boxplot(aes(x = as.factor(publication_year), y = sample_age))
```

- This is not well readable, because extreme outliers dictate the scale

## `scale`s control the behaviour of visual elements

- We can change the **scale** of different visual elements - e.g. the y-axis

```{r}
ggplot(samples) +
geom_boxplot(aes(x = as.factor(publication_year), y = sample_age)) +
scale_y_log10()
```

- The log-scale improves readability

## `scale`s control the behaviour of visual elements

- (Fill) color is a visual element of the plot and its scaling can be adjusted

```{r}
ggplot(samples) +
geom_boxplot(aes(x = as.factor(publication_year), y = sample_age,
                 fill = as.factor(publication_year))) +
scale_y_log10() + scale_fill_viridis_d(option = "C")
```

## Defining plot matrices via `facet`s

- Splitting up the plot by categories into **facets** is another way to visualize more variables at once

```{r}
ggplot(samples) +
geom_count(aes(x = as.factor(publication_year), y = material)) +
facet_wrap(~archive)
```

- Unfortunately the x-axis became unreadable

## Setting purely aesthetic settings with `theme`

- Aesthetic changes like this can be applied as part of the `theme`

```{r}
ggplot(samples) +
geom_count(aes(x = as.factor(publication_year), y = material)) +
facet_wrap(~archive) +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
```

## Exercise 1

# Conditional queries on tibbles

## Selecting columns and filtering rows with `select` and `filter`

- The `dplyr` package includes powerful functions to subset data in tibbles based on conditions
- `dplyr::select` allows to select columns

```{r}
dplyr::select(samples, project_name, sample_age)   # reduce to two columns
dplyr::select(samples, -project_name, -sample_age) # remove two columns 
```

- `dplyr::filter` allows for conditional filtering of rows

```{r}
dplyr::filter(samples, publication_year == 2014)  # samples published in 2014
dplyr::filter(samples, publication_year == 2014 |
                       publication_year == 2018)  # samples from 2015 OR 2018
dplyr::filter(samples, publication_year %in% c(2014, 2018)) # match operator: %in%
dplyr::filter(samples, sample_host == "Homo sapiens" & 
                       community_type == "oral")  # oral samples from modern humans
```


## Chaining functions together with the pipe `%>%`

- The pipe `%>%` in the `magrittr` package is a clever infix operator to chain data and operations

```{r}
library(magrittr)
samples %>% dplyr::filter(publication_year == 2014)
```

- It *pipes* the LHS *in* as the first argument of the function appearing on the RHS
- That allows for sequences of functions ("tidyverse style")

```{r}
samples %>%
  dplyr::select(sample_host, community_type) %>%
  dplyr::filter(sample_host == "Homo sapiens" & community_type == "oral") %>%
  nrow() # count the rows
```

- `magrittr` also offers some more operators, among which the extraction `%$%` is particularly useful

```{r}
samples %>%
  dplyr::filter(material == "tooth") %$%
  sample_age %>% # extract the sample_age column as a vector
  max()          # get the maximum of said vector
```

## Summary statistics in `base` R

- Summarising and counting data is indispensable and R offers all operations you would expect in its `base` package

```{r}
nrow(samples)              # number of rows in a tibble
length(samples$site_name)  # length/size of a vector
unique(samples$material)   # unique elements of a vector

min(samples$sample_age)    # minimum
max(samples$sample_age)    # maximum

mean(samples$sample_age)   # mean
median(samples$sample_age) # median

var(samples$sample_age)    # variance
sd(samples$sample_age)     # standard deviation
quantile(samples$sample_age, probs = 0.75) # sample quantiles for the given probs
```

- many of these functions can ignore missing values with an option `na.rm = TRUE`

## Group-wise summaries with `group_by` and `summarise`

- These summary statistics are particular useful when applied to conditional subsets of a dataset
- `dplyr` allows such summary operations with a combination of `group_by` and `summarise`

```{r}
samples %>%
  dplyr::group_by(material) %>%  # group the tibble by the material column
  dplyr::summarise(
    min_age = min(sample_age),   # a new column: min age for each group
    median_age = median(sample_age), # a new column: median age for each group
    max_age = max(sample_age)    # a new column: max age for each group
  )
```

- grouping can be applied across multiple columns

```{r}
samples %>%
  dplyr::group_by(material, sample_host) %>% # group by material and host
  dplyr::summarise(
    n = dplyr::n(),  # a new column: number of samples for each group
    .groups = "drop" # drop the grouping after this summary operation
  )
```

## Sorting and slicing tibbles with `arrange` and `slice`

- `dplyr` allows to `arrange` tibbles by one or multiple columns

```{r}
samples %>% dplyr::arrange(publication_year)        # sort by publication year
samples %>% dplyr::arrange(publication_year,
                           sample_age)              # ... and sample age
samples %>% dplyr::arrange(dplyr::desc(sample_age)) # sort descending on sample age
```

- Sorting also works within groups and can be paired with `slice` to extract extreme values per group

```{r}
samples %>%
  dplyr::group_by(publication_year) %>%       # group by publication year
  dplyr::arrange(dplyr::desc(sample_age)) %>% # sort by age within (!) groups
  dplyr::slice_head(n = 2) %>%                # keep the first two samples per group
  dplyr::ungroup()                            # remove the still lingering grouping
```

- Slicing is also the relevant operation to take random samples from the observations in a tibble

```{r}
samples %>% dplyr::slice_sample(n = 20)
```

## Exercise 2

# Transforming and manipulating tibbles

## Renaming and reordering columns and values with `rename`, `relocate` and `recode`

- Columns in tibbles can be renamed with `dplyr::rename` and reordered with `dplyr::relocate`

```{r}
samples %>% dplyr::rename(country = geo_loc_name) # rename a column
samples %>% dplyr::relocate(site_name, .before = project_name) # reorder columns
```

- Values in columns can also be changed with `dplyr::recode`

```{r}
samples$sample_host %>% dplyr::recode(`Homo sapiens` = "modern human")
```

- R supports explicitly ordinal data with `factor`s, which can be reordered as well
- `factor`s can be handeld more easily with the `forcats` package

```{r, fig.show='hide'}
ggplot(samples) + geom_bar(aes(x = community_type)) # bars are alphabetically ordered
sa2 <- samples
sa2$cto <- forcats::fct_reorder(sa2$community_type, sa2$community_type, length)
# fct_reorder: reorder the input factor by a summary statistic on an other vector
ggplot(sa2) + geom_bar(aes(x = community_type)) # bars are ordered by size
```

## Adding columns to tibbles with `mutate` and `transmute`

- A common application of data manipulation is adding derived columns. `dplyr` offers that with `mutate`

```{r}
samples %>% 
  dplyr::mutate(                                               # add a column that
    archive_summary = paste0(archive, ": ", archive_accession) # combines two other
  ) %$% archive_summary                                        # columns
```

- `dplyr::transmute` removes all columns but the newly created ones

```{r}
samples %>% 
  dplyr::transmute(
    sample_name = tolower(sample_name), # overwrite this columns
    publication_doi                     # select this column
)
```

- `tibble::add_column` behaves as `dplyr::mutate`, but gives more control over column position

```{r}
samples %>% tibble::add_column(., id = 1:nrow(.), .before = "project_name")
```

## Conditional operations with `ifelse` and `case_when`

- `ifelse` allows to implement conditional `mutate` operations, that consider information from other columns, but that gets cumbersome easily

```{r}
samples %>% dplyr::mutate(hemi = ifelse(latitude >= 0, "North", "South")) %$% hemi
```

```{r}
samples %>% dplyr::mutate(
  hemi = ifelse(is.na(latitude), "unknown", ifelse(latitude >= 0, "North", "South"))
) %$% hemi
```

- `dplyr::case_when` is a much more readable solution for this application

```{r}
samples %>% dplyr::mutate(
  hemi = dplyr::case_when(
    latitude >= 0 ~ "North",
    latitude < 0  ~ "South",
    TRUE          ~ "unknown" # TRUE catches all remaining cases
  )
) %$% hemi
```

## Long and wide data formats

- For different applications or to simplify certain analysis or plotting operations data often has to be transformed from a **wide** to a **long** format or vice versa

![](figures/pivot_longer_wider.png){height=80px}

- A table in **wide** format has N key columns and N value columns
- A table in **long** format has N key columns, one descriptor column and one value column

## A wide dataset

```{r}
carsales <- tibble::tribble(
  ~brand, ~`2014`, ~`2015`, ~`2016`, ~`2017`,
  "BMW",  20,      25,      30,      45,
  "VW",   67,      40,     120,      55
)
```

```{r, results="markup", echo=FALSE}
carsales
```

- Wide format becomes a problem, when the columns are semantically identical. This dataset is in wide format and we can not easily plot it
- We generally prefer data in long format, although it is more verbose with more duplication. "Long" format data is more "tidy"

## Making a wide dataset long with `pivot_longer`

```{r}
carsales_long <- carsales %>% tidyr::pivot_longer(
  cols = tidyselect::num_range("", range = 2014:2017), # set of columns to transform
  names_to = "year",            # the name of the descriptor column we want
  names_transform = as.integer, # a transformation function to apply to the names
  values_to = "sales"           # the name of the value column we want
)
```

```{r, results="markup", echo=FALSE}
carsales_long
```

## Making a long dataset wide with `pivot_wider`

```{r}
carsales_wide <- carsales_long %>% tidyr::pivot_wider(
  id_cols = "brand",  # the set of id columns that should not be changed
  names_from = year,  # the descriptor column with the names of the new columns
  values_from = sales # the value column from which the values should be extracted
)
```

```{r, results="markup", echo=FALSE}
carsales_wide
```

- Applications of wide datasets are adjacency matrices to represent graphs, covariance matrices or other pairwise statistics
- When data gets big, then wide formats can be significantly more efficient (e.g. for spatial data)

## Exercise 3

# Combining tibbles with join operations

## Types of joins

Joins combine two datasets x and y based on key columns

- Mutating joins add columns from one dataset to the other
  - Left join: Take observations from x and add fitting information from y
  - Right join: Take observations from y and add fitting information from x
  - Inner join: Join the overlapping observations from x and y
  - Full join: Join all observations from x and y, even if information is missing
- Filtering joins remove observations from x based on their presence in y
  - Semi join: Keep every observation in x that is in y
  - Anti join: Keep every observation in x that is not in y

## A second dataset

```{r echo=FALSE}
library_table_url <- "https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/ancientmetagenome-hostassociated/libraries/ancientmetagenome-hostassociated_libraries.tsv"
```

```{r, results='markup'}
libraries <- readr::read_tsv(library_table_url)
print(libraries, n = 3)
```

## Meaningful subsets

```{r echo=FALSE}
samsub <- samples %>% dplyr::select(project_name, sample_name, sample_age)
libsub <- libraries %>% dplyr::select(project_name, sample_name, library_name, read_count)
```

```{r, results='markup'}
print(samsub, n = 3)
print(libsub, n = 3)
```

## Left join

Take observations from x and add fitting information from y

![](figures/left_join.png){height=70px}

```{r}
left <- dplyr::left_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name") # the key columns by which to join
)
```

```{r, echo=FALSE, results='markup'}
print(left, n = 1)
```

- Left joins are the most common join operation: Add information from another dataset

## Right join

Take observations from y and add fitting information from x

![](figures/right_join.png){height=70px}

```{r}
right <- dplyr::right_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name")
)
```

```{r, echo=FALSE, results='markup'}
print(right, n = 1)
```

- Right joins are almost identical to left joins -- only x and y have reversed roles

## Inner join

Join the overlapping observations from x and y

![](figures/inner_join.png){height=70px}

```{r}
inner <- dplyr::inner_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name")
)
```

```{r, echo=FALSE, results='markup'}
print(inner, n = 1)
```

- Inner joins are a fast and easy way to check, to which degree two dataset overlap

## Full join

Join all observations from x and y, even if information is missing

![](figures/full_join.png){height=70px}

```{r}
full <- dplyr::full_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name")
)
```

```{r, echo=FALSE, results='markup'}
print(full, n = 1)
```

- Full joins allow to preserve every bit of information

## Semi join

Keep every observation in x that is in y

![](figures/semi_join.png){height=70px}

```{r}
semi <- dplyr::semi_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name")
)
```

```{r, echo=FALSE, results='markup'}
print(semi, n = 1)
```

- Semi joins are underused operations to filter datasets

## Anti join

Keep every observation in x that is not in y

![](figures/anti_join.png){height=70px}

```{r}
anti <- dplyr::anti_join(
  x = samsub,                           # 1060 observations
  y = libsub,                           # 1657 observations
  by = c("project_name", "sample_name")
)
```

```{r, echo=FALSE, results='markup'}
print(anti, n = 1)
```

- Anti joins allow to quickly specify incomplete datasets and missing information

## Exercise 4
